{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to configure a laptop's NVIDIA GPU for Deep Learning\n",
    "> Use your own Graphics Card to speed up Tensorflow\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [GPU, tensorflow, torch]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is well known for anybody who has ever tried to train any Deep Learning model beyond a simple Multi Layer Perceptron on a few observations that using a GPU for training is kind of a must. While nowadays many different platforms and services offer GPUs for free (I'm looking at you Kaggle and Google Colab) if you're anything like me, and you also happen to have a good NVIDIA Graphics Card in your computer, surely you've felt like giving it a go by yourself.\n",
    "\n",
    "And here is where complications start. Configuring a NVIDIA GPU to be found by your favourite Deep Learning framework it's way more complicated than it should. Specially if you are installing it on Windows, as it happens to me. Chances are that you have not purchased a high-end NVIDIA exclusively for doing Deep Learning, bur rather you bought the laptop as a gaming station (this is basically what happened to me) and thought of exploring the GPU capabilities in it.\n",
    "\n",
    "I've gone through the process a few times, and what follows it's my own personal guidelines to replicate the setup should I have to do a hard reset on my laptop (God forbids),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we need to install a few things, and the bigger complexity comes from knowing what these are, and which version we should get.\n",
    "\n",
    "First of all, we are going to download the [latest driver](https://www.nvidia.es/Download/index.aspx) for Our NVIDIA Card. In my case, it's a RTX 2060 operating on Windows 11. For this one you would want to have the latest one. For things we will be installing afterwwards that won't be the case, as we will have contraints due to for example, which version of CUDA is allowed for our card model.\n",
    "\n",
    "Another thing you would need is the latest **Microsoft Visual C++** development kit.\n",
    "\n",
    "Install [CUDA toolkit 11](https://developer.nvidia.com/cuda-downloads) at least, and certainly the latest one available.\n",
    "\n",
    "We will also need **cuDNN**. Here we differ from the guidelines offered by Professor Heaton in the tutorial in the references section. Reason is, if we are willing to go with versions that are not precisely the latest ones for Tensorflow or other frameworks, then we can let **conda** take care of the installation of the rest of pieces. We will use conda to create a brand new environment containing a version of cudnn and cudatoolkit that our Graphics Card would tolerate, and finally a good combination of python version and tensorflow version. \n",
    "\n",
    "In my case this happens to be:\n",
    "\n",
    "```bash\n",
    "conda create -n tensorflow-cuda cudnn=7.6 cudatoolkit=11.1 python=3.7\n",
    "conda activate tensorflow-cuda\n",
    "conda install nb_conda\n",
    "pip install tensorflow==2.3.0\n",
    "conda env update --file environment.yml\n",
    "python -m ipykernel install --user --name tensorflow-cuda --display-name \"Python (tensorflow-cuda)\"\n",
    "```\n",
    "\n",
    "I typically use `environment.yml` as a mean to install any additional library I may need and update the environment on the fly each time. Say I need or want to use scikitlearn and it wasn't installed, I just add it to the `environment.yml` file and update the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can quickly test whether our installation of Tensorflow is working appropiately. I will use the jupyter kernel running behind this post as it is easy and convenient for me. A simple matrix multiplication will do. I mean, that's **why we use GPUs to begin with**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alvaroof\\AppData\\Local\\Temp\\ipykernel_14080\\712885346.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I switch to the jupyter kernel without GPU enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Wall time: 6.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.debugging.set_log_device_placement(True) \n",
    "a = tf.constant([1.22582], shape=[10000, 10000], name='a')\n",
    "b = tf.constant([1.1125], shape=[10000, 10000], name='b')\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.debugging.set_log_device_placement(True) \n",
    "a = tf.constant([1.22582], shape=[10000, 10000], name='a')\n",
    "b = tf.constant([1.1125], shape=[10000, 10000], name='b')\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with GPU the matrix multiplication takes 10x less time to run :smiley:."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Tensorflow Documentation](https://www.tensorflow.org/install/gpu)\n",
    "* [Jeff Heaton's Video Tutorial](https://www.youtube.com/watch?v=OEFKlRSd8Ic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-cuda",
   "language": "python",
   "name": "tensorflow-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
